\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{garthwaite2005,ohagan2006}
\citation{berger2006}
\citation{kass1995}
\gdef\hy@title{Bayesian Restricted Likelihood Methods: Conditioning on Insufficient Statistics in Bayesian Regression}
\thanksnewlabel{T1thanks}{{\TextOrMath  {\textasteriskcentered }{*}}{1}}
\thanksnewlabel{e1@email}{{lewis.865@osu.edu}{1}}
\thanksnewlabel{e2@email}{{snm@stat.osu.edu}{1}}
\thanksnewlabel{e3@email}{{yklee@stat.osu.edu}{1}}
\thanksnewlabel{addr1thanks}{{\TextOrMath  {\textdagger }{\dagger }}{1}}
\gdef\hy@author{John R. Lewis, Steven N. MacEachern and Yoonkyung Lee}
\gdef\hy@subject{Bayesian Analysis0000 0001}
\gdef\hy@keywords{Markov chain Monte Carlo, M-estimation, Robust regression}
\gdef\author@num{3}
\citation{yuan1999minimally}
\citation{zhu2011}
\citation{clyde2004}
\citation{bernardo2000,clyde2013,clarke2013Complete}
\citation{lee2014}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\@writefile{brf}{\backcite{garthwaite2005}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{ohagan2006}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{berger2006}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{kass1995}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{yuan1999minimally}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{zhu2011}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{clyde2004}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{bernardo2000}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{clyde2013}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{clarke2013Complete}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{lee2014}{{2}{1}{section.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Restricted Likelihood}{3}{section.2}}
\newlabel{restrictedlikelihood}{{2}{3}{Restricted Likelihood}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Examples}{3}{subsection.2.1}}
\citation{ratcliff1993}
\citation{lewis2014}
\newlabel{OutlyingCases}{{1}{4}{Examples}{equation.2.1}{}}
\@writefile{brf}{\backcite{ratcliff1993}{{4}{2.1}{equation.2.1}}}
\newlabel{Censoring}{{2}{4}{Examples}{equation.2.2}{}}
\@writefile{brf}{\backcite{lewis2014}{{4}{2.1}{equation.2.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Generalization}{4}{subsection.2.2}}
\newlabel{FullLikelihood}{{3}{4}{Generalization}{equation.2.3}{}}
\citation{wong2004}
\citation{savage1969}
\citation{pettitt1983,pettitt1982}
\citation{hoff2013}
\citation{lewis2012}
\citation{doksum1990}
\citation{clarke1995}
\citation{yuan2004}
\citation{hwang2005}
\citation{pratt1965}
\citation{tavare1997,pritchard1999,marjoram2003,fearnhead2012}
\citation{joyce2008}
\newlabel{RestrictedPosterior}{{4}{5}{Generalization}{equation.2.4}{}}
\newlabel{RestrictedpredDist}{{5}{5}{Generalization}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Literature review}{5}{subsection.2.3}}
\@writefile{brf}{\backcite{wong2004}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{savage1969}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{pettitt1983}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{pettitt1982}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{hoff2013}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{lewis2012}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{doksum1990}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{clarke1995}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{yuan2004}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{hwang2005}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{pratt1965}{{5}{2.3}{subsection.2.3}}}
\citation{stigler1977}
\citation{lee2014}
\@writefile{brf}{\backcite{tavare1997}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{pritchard1999}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{marjoram2003}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{fearnhead2012}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{joyce2008}{{6}{2.3}{subsection.2.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Illustrative Examples}{6}{section.3}}
\newlabel{illustrations}{{3}{6}{Illustrative Examples}{section.3}{}}
\@writefile{brf}{\backcite{stigler1977}{{6}{3}{section.3}}}
\citation{huber2009}
\@writefile{brf}{\backcite{lee2014}{{7}{3}{section.3}}}
\@writefile{brf}{\backcite{huber2009}{{7}{3}{equation.3.6}}}
\citation{rousseeuw1987}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Results from the analysis of the speed of light data. Top: Posterior distributions of $\beta $ under each model. Bottom: Log posterior predictive distributions under each model. The differences in the tails are emphasized in the bottom plot. The horizontal axis is strategically labeled to help compare the centers of the distributions in each of the plots.}}{8}{figure.1}}
\newlabel{fig:newcomb_post}{{1}{8}{Results from the analysis of the speed of light data. Top: Posterior distributions of $\beta $ under each model. Bottom: Log posterior predictive distributions under each model. The differences in the tails are emphasized in the bottom plot. The horizontal axis is strategically labeled to help compare the centers of the distributions in each of the plots}{figure.1}{}}
\@writefile{brf}{\backcite{rousseeuw1987}{{8}{3}{figure.1}}}
\citation{kass1995reference}
\@writefile{brf}{\backcite{kass1995reference}{{9}{3}{equation.3.7}}}
\citation{huber1964}
\citation{lewis2014}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Pointwise posterior predictive intervals of log(calls) under the normal theory model fit to the non-outliers, the restricted likelihood model with Tukey's M-estimator for the slope and intercept with Huber's `proposal 2' for scale, and a heavy-tailed t-distribution model. The first three data points were used to specify the prior with each model using the remaining 21 for fitting. The normal theory model was also fit after removing observations 14-20 (years 1963 - 1970).}}{10}{figure.2}}
\newlabel{fig:calls_predictive}{{2}{10}{Pointwise posterior predictive intervals of log(calls) under the normal theory model fit to the non-outliers, the restricted likelihood model with Tukey's M-estimator for the slope and intercept with Huber's `proposal 2' for scale, and a heavy-tailed t-distribution model. The first three data points were used to specify the prior with each model using the remaining 21 for fitting. The normal theory model was also fit after removing observations 14-20 (years 1963 - 1970)}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Restricted Likelihood for the Linear Model}{10}{section.4}}
\newlabel{BayesLinMod}{{4}{10}{Restricted Likelihood for the Linear Model}{section.4}{}}
\@writefile{brf}{\backcite{huber1964}{{10}{4}{section.4}}}
\@writefile{brf}{\backcite{lewis2014}{{11}{4}{section.4}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}The Bayesian linear model}{11}{subsection.4.1}}
\newlabel{LinearModel}{{8}{11}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{fullRank}{{C1}{11}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{supReal}{{C2}{11}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{asb}{{C3}{11}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{as}{{C4}{11}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{regEq}{{C5}{11}{The Bayesian linear model}{equation.4.8}{}}
\citation{huber2009,maronna2006}
\citation{lewis2014}
\citation{gelfand1990}
\citation{liu1994,liang2008}
\citation{hastings1970}
\newlabel{scaleEqReg}{{C6}{12}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{regIn}{{C7}{12}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{scaleEq2Reg}{{C8}{12}{The Bayesian linear model}{equation.4.8}{}}
\@writefile{brf}{\backcite{huber2009}{{12}{4.1}{equation.4.8}}}
\@writefile{brf}{\backcite{maronna2006}{{12}{4.1}{equation.4.8}}}
\@writefile{brf}{\backcite{lewis2014}{{12}{4.1}{equation.4.8}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Computational strategy}{12}{subsection.4.2}}
\newlabel{highDim}{{4.2}{12}{Computational strategy}{subsection.4.2}{}}
\@writefile{brf}{\backcite{gelfand1990}{{12}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{liu1994}{{12}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{liang2008}{{12}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{hastings1970}{{12}{4.2}{subsection.4.2}}}
\newlabel{MHRatio}{{9}{13}{Computational strategy}{equation.4.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Construction of the proposal}{13}{section*.1}}
\newlabel{Transformation}{{4.1}{13}{}{theorem.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluation of the proposal density}{15}{section*.2}}
\newlabel{gradSTheoremReg}{{4.2}{15}{}{theorem.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A depiction of $\mathcal  {A}$, $\Pi (\mathcal  {A})$, and the unit circle for the illustrative example where $b_{1}(\mathbf  {1},\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$y$})=\qopname  \relax m{min}(\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$y$})=0$ and $s(\mathbf  {1},\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$y$})=\DOTSB \sum@ \slimits@ (y_i -b_{1}(\mathbf  {1},\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$y$}))^2 =1$. $\mathcal  {A}$ is the combination of three quarter circles, one on each plane defined by $y_i=0$. The projection of this manifold onto the deviation space is depicted by the bowed triangular shape in the plane defined by $\DOTSB \sum@ \slimits@ y_i=0$. The circle in this plane represents the sample space for the intermediate sample $\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$z$}^*$. Also depicted is the vector $\mathbf  {1}$, the design matrix for the location and scale setting.}}{16}{figure.3}}
\newlabel{fig:sampSpace}{{3}{16}{A depiction of $\mathcal {A}$, $\Pi (\mathcal {A})$, and the unit circle for the illustrative example where $b_{1}(\mathbf {1},\by )=\min (\by )=0$ and $s(\mathbf {1},\by )=\sum (y_i -b_{1}(\mathbf {1},\by ))^2 =1$. $\mathcal {A}$ is the combination of three quarter circles, one on each plane defined by $y_i=0$. The projection of this manifold onto the deviation space is depicted by the bowed triangular shape in the plane defined by $\sum y_i=0$. The circle in this plane represents the sample space for the intermediate sample $\bz ^*$. Also depicted is the vector $\mathbf {1}$, the design matrix for the location and scale setting}{figure.3}{}}
\newlabel{cosine}{{12}{16}{Evaluation of the proposal density}{equation.4.12}{}}
\newlabel{fig:stretchDeform}{{4.2}{17}{Evaluation of the proposal density}{equation.4.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Visualization of the scaling from $z^{*}$ to $z$. Left: the first substep scales $z^{*}$ on the unit circle to the circle of radius $r = ||z||$; resulting in a change-of-variables transformation for the unit circle to that of radius $r$. The contribution to the Jacobian of this transformation is $r^{-(n-p-1)}$. Right: The second substep accounts for the the change-of-variables transformation from the circle of radius $r$ to $\Pi (\mathcal  {A})$. The normal vectors to these two sets are used to calculated the contribution to the Jacobian of this part of the transformation are shown in the figure.}}{17}{figure.4}}
\newlabel{lem:basis}{{4.3}{17}{}{theorem.4.3}{}}
\citation{miao1992}
\citation{miao1992}
\newlabel{lem:fullrank}{{4.4}{18}{}{theorem.4.4}{}}
\newlabel{eq:volume}{{13}{18}{Evaluation of the proposal density}{equation.4.13}{}}
\@writefile{brf}{\backcite{miao1992}{{18}{4.2}{equation.4.13}}}
\newlabel{Jacobian}{{4.5}{18}{}{theorem.4.5}{}}
\newlabel{dens:ystst}{{14}{18}{}{equation.4.14}{}}
\@writefile{brf}{\backcite{miao1992}{{19}{4.2}{equation.4.14}}}
\newlabel{theorem:sings}{{4.7}{19}{}{theorem.4.7}{}}
\newlabel{Mest}{{15}{19}{Evaluation of the proposal density}{equation.4.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Simulated Data}{19}{section.5}}
\newlabel{simData}{{5}{19}{Simulated Data}{section.5}{}}
\newlabel{gensim2}{{16}{19}{Simulated Data}{equation.5.16}{}}
\citation{huber2009}
\newlabel{fullsim2}{{17}{20}{Simulated Data}{equation.5.17}{}}
\@writefile{brf}{\backcite{huber2009}{{20}{5}{equation.5.17}}}
\citation{huber2009}
\newlabel{kl}{{18}{21}{Simulated Data}{equation.5.18}{}}
\newlabel{kl_sim}{{5}{22}{Simulated Data}{equation.5.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Average KL - divergence plus/minus one standard error for each value of $a_{s}$ and $c$ ($\overline  {KL}^{(M)}_{{\cdot }{\cdot }} \pm SE(\overline  {KL}^{(M)}_{{\cdot } k})$). Smaller values represent better fits. The panels correspond to $c = 0.5$ (left), $c=1$ (middle), and $c=2$ (right) with the values of $a_{s}$ on the horizontal axis. The average KL for the normal theory model ranges between $0.31$ and $0.37$ and are left out of the figure.}}{22}{figure.5}}
\@writefile{brf}{\backcite{huber2009}{{22}{5}{equation.5.18}}}
\citation{zellner1986,liang2008}
\citation{kass1995reference}
\@writefile{toc}{\contentsline {section}{\numberline {6}Real Data}{23}{section.6}}
\newlabel{RealData}{{6}{23}{Real Data}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}State Level Regression model}{23}{subsection.6.1}}
\newlabel{regModelNW}{{6.1}{23}{State Level Regression model}{subsection.6.1}{}}
\newlabel{eq:regModel}{{19}{23}{State Level Regression model}{equation.6.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The square root of count in 2012 versus that in 2010 for four states. The colors represent the varying contractual agreements as they stood in 2010 (`Type'). Agencies that closed during the 2010-2012 period are represented by the zero counts for 2012.}}{24}{figure.6}}
\newlabel{fig:ctVct}{{6}{24}{The square root of count in 2012 versus that in 2010 for four states. The colors represent the varying contractual agreements as they stood in 2010 (`Type'). Agencies that closed during the 2010-2012 period are represented by the zero counts for 2012}{figure.6}{}}
\@writefile{brf}{\backcite{zellner1986}{{24}{6.1}{equation.6.19}}}
\@writefile{brf}{\backcite{liang2008}{{24}{6.1}{equation.6.19}}}
\@writefile{brf}{\backcite{kass1995reference}{{24}{6.1}{equation.6.19}}}
\citation{ronchetti1997}
\citation{jung2014}
\@writefile{toc}{\contentsline {subsubsection}{Method of model comparison}{25}{section*.3}}
\@writefile{brf}{\backcite{ronchetti1997}{{25}{6.1}{section*.3}}}
\@writefile{brf}{\backcite{jung2014}{{25}{6.1}{section*.3}}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison of predictive performance}{26}{section*.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Hierarchical regression model}{27}{subsection.6.2}}
\newlabel{hierRegNW}{{6.2}{27}{Hierarchical regression model}{subsection.6.2}{}}
\newlabel{eq:hierModel}{{20}{27}{Hierarchical regression model}{equation.6.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Average TLM plus/minus one standard deviation over $K = 50$ splits into training and holdout samples. The panels are for the different states 2, 15, 27, and 36, with $n = 222, 40, 117,$ and $46$, respectively. The horizontal axis is the percent of $n$ used in each training set. The color corresponds to the fitting model. Larger values of TLM are better.}}{28}{figure.7}}
\newlabel{fig:tlm}{{7}{28}{Average TLM plus/minus one standard deviation over $K = 50$ splits into training and holdout samples. The panels are for the different states 2, 15, 27, and 36, with $n = 222, 40, 117,$ and $46$, respectively. The horizontal axis is the percent of $n$ used in each training set. The color corresponds to the fitting model. Larger values of TLM are better}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Average TLM plus/minus one standard deviation over $K = 50$ splits into training and holdout samples for several values of the trimming fraction $\alpha $. The training sample size used is $0.5n$. Larger values of TLM are better.}}{29}{figure.8}}
\newlabel{fig:tlmbyAlpha}{{8}{29}{Average TLM plus/minus one standard deviation over $K = 50$ splits into training and holdout samples for several values of the trimming fraction $\alpha $. The training sample size used is $0.5n$. Larger values of TLM are better}{figure.8}{}}
\citation{gelman2006}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Hierarchical model results: $\overline  {TLM}_b(A)_{\cdot }$ plus/minus one standard deviation over $K = 50$ splits into training and holdout sets with the Student-t as the base model and several values of the trimming fraction $\alpha $. Larger values of TLM are better.}}{31}{figure.9}}
\newlabel{fig:hierTLM}{{9}{31}{Hierarchical model results: $\overline {TLM}_b(A)_{\cdot }$ plus/minus one standard deviation over $K = 50$ splits into training and holdout sets with the Student-t as the base model and several values of the trimming fraction $\alpha $. Larger values of TLM are better}{figure.9}{}}
\@writefile{brf}{\backcite{gelman2006}{{31}{6.2}{figure.10}}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Hierarchical model results: standard deviation of $\overline  {TLM}_b(A)_{\cdot }$ relative to the mean over $K = 50$ splits into training and holdout sets for several values of the trimming fraction $\alpha $. Results are given for the two restricted likelihood versions of the hierarchical model and their corresponding robust regression models fit separately within each state.}}{32}{figure.10}}
\newlabel{fig:hierTLMsd}{{10}{32}{Hierarchical model results: standard deviation of $\overline {TLM}_b(A)_{\cdot }$ relative to the mean over $K = 50$ splits into training and holdout sets for several values of the trimming fraction $\alpha $. Results are given for the two restricted likelihood versions of the hierarchical model and their corresponding robust regression models fit separately within each state}{figure.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{32}{section.7}}
\newlabel{Conclusions}{{7}{32}{Discussion}{section.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Appendix}{34}{section.8}}
\newlabel{sec:appendix}{{8}{34}{Appendix}{section.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Proofs}{34}{subsection.8.1}}
\newlabel{perpGradReg}{{8.1}{35}{Proofs}{theorem.8.1}{}}
\newlabel{eq:lem3.2}{{26}{35}{Proofs}{equation.8.26}{}}
\citation{miao1992}
\newlabel{BigMatrix}{{27}{36}{Proofs}{equation.8.27}{}}
\@writefile{brf}{\backcite{miao1992}{{36}{8.1}{equation.8.27}}}
\newlabel{MBI:lemma}{{8.2}{36}{}{theorem.8.2}{}}
\newlabel{MBI:thm}{{8.3}{37}{}{theorem.8.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Setting the hierarchical prior values}{37}{subsection.8.2}}
\bibstyle{bib/ba}
\bibdata{bib/refPaper1}
\bibcite{berger2006}{{1}{2006}{{Berger}}{{}}}
\bibcite{bernardo2000}{{2}{2000}{{Bernardo and Smith}}{{}}}
\bibcite{clarke1995}{{3}{1995}{{Clarke and Ghosh}}{{}}}
\bibcite{clarke2013Complete}{{4}{2013}{{Clarke et~al.}}{{Clarke, Clarke, Yu et~al.}}}
\bibcite{clyde2004}{{5}{2004}{{Clyde and George}}{{}}}
\bibcite{clyde2013}{{6}{2013}{{Clyde and Iversen}}{{}}}
\bibcite{doksum1990}{{7}{1990}{{Doksum and Lo}}{{}}}
\bibcite{fearnhead2012}{{8}{2012}{{Fearnhead and Prangle}}{{}}}
\bibcite{garthwaite2005}{{9}{2005}{{Garthwaite et~al.}}{{Garthwaite, Kadane, and O'Hagan}}}
\bibcite{gelfand1990}{{10}{1990}{{Gelfand and Smith}}{{}}}
\@writefile{toc}{\contentsline {section}{References}{38}{section*.6}}
\bibcite{gelman2006}{{11}{2006}{{Gelman}}{{}}}
\bibcite{hastings1970}{{12}{1970}{{Hastings}}{{}}}
\bibcite{hoff2013}{{13}{2013}{{Hoff et~al.}}{{Hoff, Fosdick, Volfovsky, and Stovel}}}
\bibcite{huber2009}{{14}{2009}{{Huber and Ronchetti}}{{}}}
\bibcite{huber1964}{{15}{1964}{{Huber}}{{}}}
\bibcite{hwang2005}{{16}{2005}{{Hwang et~al.}}{{Hwang, So, and Kim}}}
\bibcite{joyce2008}{{17}{2008}{{Joyce and Marjoram}}{{}}}
\bibcite{jung2014}{{18}{2014}{{Jung et~al.}}{{Jung, MacEachern, and Lee}}}
\bibcite{kass1995}{{19}{1995}{{Kass and Raftery}}{{}}}
\bibcite{kass1995reference}{{20}{1995}{{Kass and Wasserman}}{{}}}
\bibcite{lee2014}{{21}{2014}{{Lee and MacEachern}}{{}}}
\bibcite{lewis2014}{{22}{2014}{{Lewis}}{{}}}
\bibcite{lewis2012}{{23}{2012}{{Lewis et~al.}}{{Lewis, Lee, and MacEachern}}}
\bibcite{liang2008}{{24}{2008}{{Liang et~al.}}{{Liang, Paulo, Molina, Clyde, and Berger}}}
\bibcite{liu1994}{{25}{1994}{{Liu}}{{}}}
\bibcite{marjoram2003}{{26}{2003}{{Marjoram et~al.}}{{Marjoram, Molitor, Plagnol, and Tavar\'{e}}}}
\bibcite{maronna2006}{{27}{2006}{{Maronna et~al.}}{{Maronna, Martin, and Yohai}}}
\bibcite{miao1992}{{28}{1992}{{Miao and Ben-Israel}}{{}}}
\bibcite{ohagan2006}{{29}{2006}{{O'Hagan et~al.}}{{O'Hagan, Buck, Daneshkhah, Eiser, Garthwaite, Jenkinson, Oakley, and Rakow}}}
\bibcite{pettitt1982}{{30}{1982}{{Pettitt}}{{}}}
\bibcite{pettitt1983}{{31}{1983}{{Pettitt}}{{}}}
\bibcite{pratt1965}{{32}{1965}{{Pratt}}{{}}}
\bibcite{pritchard1999}{{33}{1999}{{Pritchard et~al.}}{{Pritchard, Seielstad, Perez-Lezaun, and Feldman}}}
\bibcite{ratcliff1993}{{34}{1993}{{Ratcliff}}{{}}}
\bibcite{ronchetti1997}{{35}{1997}{{Ronchetti et~al.}}{{Ronchetti, Field, and Blanchard}}}
\bibcite{rousseeuw1987}{{36}{1987}{{Rousseeuw and Leroy}}{{}}}
\bibcite{savage1969}{{37}{1969}{{Savage}}{{}}}
\bibcite{stigler1977}{{38}{1977}{{Stigler}}{{}}}
\bibcite{tavare1997}{{39}{1997}{{Tavar{\'e} et~al.}}{{Tavar{\'e}, Balding, Griffiths, and Donnelly}}}
\bibcite{wong2004}{{40}{2004}{{Wong and Clarke}}{{}}}
\bibcite{yuan2004}{{41}{2004}{{Yuan and Clarke}}{{}}}
\bibcite{yuan1999minimally}{{42}{1999}{{Yuan and Clarke}}{{}}}
\bibcite{zellner1986}{{43}{1986}{{Zellner}}{{}}}
\bibcite{zhu2011}{{44}{2011}{{Zhu et~al.}}{{Zhu, Ibrahim, and Tang}}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Hierarchical model results: ${TLM}_b(A)_{j}$ plus/minus one standard deviation over $K = 50$ repetitions for each state and $\alpha = 0.3$. The states are ordered along the $x$-axis according to number of agencies within the state (shown in parentheses). Results displayed are for the robust models using Tukey's M-estimators. Larger values of TLM are better.}}{42}{figure.11}}
\newlabel{fig:hierTLMstate}{{11}{42}{Hierarchical model results: ${TLM}_b(A)_{j}$ plus/minus one standard deviation over $K = 50$ repetitions for each state and $\alpha = 0.3$. The states are ordered along the $x$-axis according to number of agencies within the state (shown in parentheses). Results displayed are for the robust models using Tukey's M-estimators. Larger values of TLM are better}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Hierarchical model results: Standard deviation of ${TLM}_b(A)_{j}$ relative to the mean over $K = 50$ repetitions for each state and $\alpha = 0.3$. The states are ordered along the $x$-axis according to number of agencies within the state (shown in parentheses). Results displayed are for the robust models using Tukey's M-estimators.}}{43}{figure.12}}
\newlabel{fig:hierTLMstateSD}{{12}{43}{Hierarchical model results: Standard deviation of ${TLM}_b(A)_{j}$ relative to the mean over $K = 50$ repetitions for each state and $\alpha = 0.3$. The states are ordered along the $x$-axis according to number of agencies within the state (shown in parentheses). Results displayed are for the robust models using Tukey's M-estimators}{figure.12}{}}
