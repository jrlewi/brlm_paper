\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\gdef\hy@title{Bayesian Restricted Likelihood Methods: Conditioning on Insufficient Statistics in Bayesian Regression}
\thanksnewlabel{T1thanks}{{\TextOrMath  {\textasteriskcentered }{*}}{1}}
\thanksnewlabel{e1@email}{{lewis.865@osu.edu}{1}}
\thanksnewlabel{e2@email}{{snm@stat.osu.edu}{1}}
\thanksnewlabel{e3@email}{{yklee@stat.osu.edu}{1}}
\thanksnewlabel{addr1thanks}{{\TextOrMath  {\textdagger }{\dagger }}{1}}
\citation{garthwaite2005,ohagan2006}
\citation{berger2006}
\citation{kass1995}
\citation{lee2014}
\gdef\hy@author{John R. Lewis, Steven N. MacEachern and Yoonkyung Lee}
\gdef\hy@subject{Bayesian Analysis0000 0001}
\gdef\hy@keywords{Approximate Bayesian computation, Markov chain Monte Carlo, M-estimation, Robust regression}
\gdef\author@num{3}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\@writefile{brf}{\backcite{garthwaite2005}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{ohagan2006}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{berger2006}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{kass1995}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{lee2014}{{2}{1}{section.1}}}
\citation{ratcliff1993}
\@writefile{toc}{\contentsline {section}{\numberline {2}Restricted Likelihood}{3}{section.2}}
\newlabel{restrictedlikelihood}{{2}{3}{Restricted Likelihood}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Examples}{3}{subsection.2.1}}
\newlabel{OutlyingCases}{{1}{3}{Examples}{equation.2.1}{}}
\citation{lewis2014}
\@writefile{brf}{\backcite{ratcliff1993}{{4}{2.1}{equation.2.1}}}
\newlabel{Censoring}{{2}{4}{Examples}{equation.2.2}{}}
\@writefile{brf}{\backcite{lewis2014}{{4}{2.1}{equation.2.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Generalization}{4}{subsection.2.2}}
\newlabel{FullLikelihood}{{3}{4}{Generalization}{equation.2.3}{}}
\newlabel{RestrictedPosterior}{{4}{4}{Generalization}{equation.2.4}{}}
\citation{savage1969}
\citation{pettitt1983,pettitt1982}
\citation{hoff2013}
\citation{lewis2012}
\citation{doksum1990}
\citation{clarke1995}
\citation{yuan2004}
\citation{hwang2005}
\citation{pratt1965}
\citation{tavare1997,pritchard1999,marjoram2003,fearnhead2012}
\citation{joyce2008}
\newlabel{RestrictedpredDist}{{5}{5}{Generalization}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Literature review}{5}{subsection.2.3}}
\@writefile{brf}{\backcite{savage1969}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{pettitt1983}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{pettitt1982}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{hoff2013}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{lewis2012}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{doksum1990}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{clarke1995}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{yuan2004}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{hwang2005}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{pratt1965}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{tavare1997}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{pritchard1999}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{marjoram2003}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{fearnhead2012}{{5}{2.3}{subsection.2.3}}}
\citation{huber2009}
\@writefile{brf}{\backcite{joyce2008}{{6}{2.3}{subsection.2.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Illustrative Examples}{6}{section.3}}
\newlabel{illustrations}{{3}{6}{Illustrative Examples}{section.3}{}}
\@writefile{brf}{\backcite{huber2009}{{6}{3}{equation.3.6}}}
\citation{kass1995reference}
\@writefile{brf}{\backcite{kass1995reference}{{7}{3}{equation.3.7}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Results from the analysis of the speed of light data. Top: Posterior distributions of $\beta $ under each model. Bottom: Log posterior predictive distributions under each model.}}{8}{figure.1}}
\newlabel{fig:newcomb_post}{{1}{8}{Results from the analysis of the speed of light data. Top: Posterior distributions of $\beta $ under each model. Bottom: Log posterior predictive distributions under each model}{figure.1}{}}
\citation{huber1964}
\citation{lewis2014}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Predictive distribution of log(calls) under the Normal theory model fit to the non-outliers, the restricted likelihood model with Tukey's M-estimator for the slope and intercept with Huber's `proposal 2' for scale, and a heavy-tailed t-distribution model. The first three data points were used to specify the prior with the remaining points used in the posterior fits. See details in the Appendix.}}{9}{figure.2}}
\newlabel{fig:calls_predictive}{{2}{9}{Predictive distribution of log(calls) under the Normal theory model fit to the non-outliers, the restricted likelihood model with Tukey's M-estimator for the slope and intercept with Huber's `proposal 2' for scale, and a heavy-tailed t-distribution model. The first three data points were used to specify the prior with the remaining points used in the posterior fits. See details in the Appendix}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Restricted Likelihood for the Linear Model}{9}{section.4}}
\newlabel{BayesLinMod}{{4}{9}{Restricted Likelihood for the Linear Model}{section.4}{}}
\@writefile{brf}{\backcite{huber1964}{{9}{4}{section.4}}}
\@writefile{brf}{\backcite{lewis2014}{{9}{4}{section.4}}}
\citation{huber2009,maronna2006}
\citation{lewis2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}The Bayesian linear model}{10}{subsection.4.1}}
\newlabel{LinearModel}{{8}{10}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{fullRank}{{C1}{10}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{supReal}{{C2}{10}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{asb}{{C3}{10}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{as}{{C4}{10}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{regEq}{{C5}{10}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{scaleEqReg}{{C6}{10}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{regIn}{{C7}{10}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{scaleEq2Reg}{{C8}{10}{The Bayesian linear model}{equation.4.8}{}}
\citation{gelfand1990}
\citation{liu1994,liang2008}
\citation{hastings1970}
\@writefile{brf}{\backcite{huber2009}{{11}{4.1}{equation.4.8}}}
\@writefile{brf}{\backcite{maronna2006}{{11}{4.1}{equation.4.8}}}
\@writefile{brf}{\backcite{lewis2014}{{11}{4.1}{equation.4.8}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Computational strategy}{11}{subsection.4.2}}
\newlabel{highDim}{{4.2}{11}{Computational strategy}{subsection.4.2}{}}
\@writefile{brf}{\backcite{gelfand1990}{{11}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{liu1994}{{11}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{liang2008}{{11}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{hastings1970}{{11}{4.2}{subsection.4.2}}}
\newlabel{MHRatio}{{9}{12}{Computational strategy}{equation.4.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Construction of the proposal}{12}{section*.1}}
\newlabel{Transformation}{{4.1}{12}{}{theorem.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A depiction of $\mathcal  {A}$, $\Pi (\mathcal  {A})$, and the unit circle for the illustrative example where $b_{1}(\mathbf  {1},\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$y$})=\qopname  \relax m{min}(\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$y$})=0$ and $s(\mathbf  {1},\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$y$})=\DOTSB \sum@ \slimits@ (y_i -b_{1}(\mathbf  {1},\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$y$}))^2 =1$. $\mathcal  {A}$ is the combination of three quarter circles, one on each plane defined by $y_i=0$. The projection of this manifold onto the deviation space is depicted by the bowed triangular shape in the plane defined by $\DOTSB \sum@ \slimits@ y_i=0$. The circle in this plane represents the sample space for the intermediate sample $\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$z$}^*$. Also depicted is the vector $\mathbf  {1}$, the design matrix for the location and scale setting.}}{14}{figure.3}}
\newlabel{fig:sampSpace}{{3}{14}{A depiction of $\mathcal {A}$, $\Pi (\mathcal {A})$, and the unit circle for the illustrative example where $b_{1}(\mathbf {1},\by )=\min (\by )=0$ and $s(\mathbf {1},\by )=\sum (y_i -b_{1}(\mathbf {1},\by ))^2 =1$. $\mathcal {A}$ is the combination of three quarter circles, one on each plane defined by $y_i=0$. The projection of this manifold onto the deviation space is depicted by the bowed triangular shape in the plane defined by $\sum y_i=0$. The circle in this plane represents the sample space for the intermediate sample $\bz ^*$. Also depicted is the vector $\mathbf {1}$, the design matrix for the location and scale setting}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluation of the proposal density}{15}{section*.2}}
\newlabel{gradSTheoremReg}{{4.2}{15}{}{theorem.4.2}{}}
\newlabel{cosine}{{12}{15}{Evaluation of the proposal density}{equation.4.12}{}}
\newlabel{fig:stretchDeform}{{4.2}{16}{Evaluation of the proposal density}{equation.4.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces sdasfda}}{16}{figure.4}}
\newlabel{lem:basis}{{4.3}{16}{}{theorem.4.3}{}}
\newlabel{lem:fullrank}{{4.4}{16}{}{theorem.4.4}{}}
\citation{miao1992}
\citation{miao1992}
\newlabel{eq:volume}{{13}{17}{Evaluation of the proposal density}{equation.4.13}{}}
\@writefile{brf}{\backcite{miao1992}{{17}{4.2}{equation.4.13}}}
\newlabel{Jacobian}{{4.5}{17}{}{theorem.4.5}{}}
\newlabel{dens:ystst}{{14}{17}{}{equation.4.14}{}}
\@writefile{brf}{\backcite{miao1992}{{17}{4.2}{equation.4.14}}}
\citation{huber2009}
\newlabel{theorem:sings}{{4.7}{18}{}{theorem.4.7}{}}
\newlabel{Mest}{{15}{18}{Evaluation of the proposal density}{equation.4.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Simulated Data}{18}{section.5}}
\newlabel{simData}{{5}{18}{Simulated Data}{section.5}{}}
\newlabel{gensim2}{{16}{18}{Simulated Data}{equation.5.16}{}}
\newlabel{fullsim2}{{17}{18}{Simulated Data}{equation.5.17}{}}
\@writefile{brf}{\backcite{huber2009}{{19}{5}{equation.5.17}}}
\newlabel{kl}{{18}{19}{Simulated Data}{equation.5.18}{}}
\citation{huber2009}
\@writefile{brf}{\backcite{huber2009}{{20}{5}{equation.5.18}}}
\newlabel{kl_sim}{{5}{21}{Simulated Data}{equation.5.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Average KL - divergence plus/minus one standard error for each value of $a_{s}$ and $c$. The panels correspond to $c = 0.5$ (left), $c=1$ (middle), and $c=2$ (right) with the values of $a_{s}$ on the horizontal axis.}}{21}{figure.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Average KL - divergence plus/minus one standard error grouped by the factors $m$ (left), $n$ (middle), and $p$ (right)}}{22}{figure.6}}
\newlabel{kl_mnp}{{6}{22}{Average KL - divergence plus/minus one standard error grouped by the factors $m$ (left), $n$ (middle), and $p$ (right)}{figure.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Real Data}{22}{section.6}}
\newlabel{Real Data}{{6}{22}{Real Data}{section.6}{}}
\citation{zellner1986,liang2008}
\citation{kass1995reference}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Boxplots $(\theta _{i} - \mathaccentV {hat}05E\theta _{i})$ across all simulations separated by the values for $m$ (left), $n$ (middle), $p$ (right) where $\mathaccentV {hat}05E\theta _{i}$ are the classical robust estimators (Huber's and Tukey's).}}{23}{figure.7}}
\newlabel{boxTheta}{{7}{23}{Boxplots $(\theta _{i} - \hat \theta _{i})$ across all simulations separated by the values for $m$ (left), $n$ (middle), $p$ (right) where $\hat \theta _{i}$ are the classical robust estimators (Huber's and Tukey's)}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}State Level Regression model}{23}{subsection.6.1}}
\newlabel{regModelNW}{{6.1}{23}{State Level Regression model}{subsection.6.1}{}}
\newlabel{eq:regModel}{{19}{23}{State Level Regression model}{equation.6.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Boxplots of the classical robust estimators (Huber's and Tukey's) for $\sigma _{i}$ across all simulations separated by the values for $m$ (left), $n$ (middle), $p$ (right). The horizontal line at $\sigma = 2$ highlights the true standard deviation of the `good' data.}}{24}{figure.8}}
\newlabel{boxSigma}{{8}{24}{Boxplots of the classical robust estimators (Huber's and Tukey's) for $\sigma _{i}$ across all simulations separated by the values for $m$ (left), $n$ (middle), $p$ (right). The horizontal line at $\sigma = 2$ highlights the true standard deviation of the `good' data}{figure.8}{}}
\@writefile{brf}{\backcite{zellner1986}{{24}{6.1}{equation.6.19}}}
\@writefile{brf}{\backcite{liang2008}{{24}{6.1}{equation.6.19}}}
\@writefile{brf}{\backcite{kass1995reference}{{24}{6.1}{equation.6.19}}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The square root of count in 2012 versus that in 2010 (after centering and scaling). The colors represent the varying contractual agreements as they stood in 2010. Agencies that closed during the 2010-2012 period are represented by the zero counts for 2012. Scalings on the axes are purposely left off for proprietary reasons.}}{25}{figure.9}}
\newlabel{fig:ctVct}{{9}{25}{The square root of count in 2012 versus that in 2010 (after centering and scaling). The colors represent the varying contractual agreements as they stood in 2010. Agencies that closed during the 2010-2012 period are represented by the zero counts for 2012. Scalings on the axes are purposely left off for proprietary reasons}{figure.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Method of model comparison}{25}{section*.3}}
\citation{ronchetti1997}
\citation{jung2014}
\@writefile{brf}{\backcite{ronchetti1997}{{26}{6.1}{section*.3}}}
\@writefile{brf}{\backcite{jung2014}{{26}{6.1}{section*.3}}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison of predictive performance}{26}{section*.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Average TLM plus/minus one standard deviation over $K = 50$ splits into training and holdout samples. The panels are for the different states 2, 15, 27, and 36, with $n = 222, 40, 117,$ and $46$, respectively. The horizontal axis is the percent of $n$ used in each training set. The color corresponds to the fitting model. }}{28}{figure.10}}
\newlabel{fig:tlm}{{10}{28}{Average TLM plus/minus one standard deviation over $K = 50$ splits into training and holdout samples. The panels are for the different states 2, 15, 27, and 36, with $n = 222, 40, 117,$ and $46$, respectively. The horizontal axis is the percent of $n$ used in each training set. The color corresponds to the fitting model}{figure.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Hierarchical regression model}{28}{subsection.6.2}}
\newlabel{hierRegNW}{{6.2}{28}{Hierarchical regression model}{subsection.6.2}{}}
\newlabel{eq:hierModel}{{20}{28}{Hierarchical regression model}{equation.6.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Average TLM plus/minus one standard deviation over $K = 50$ splits into training and holdout samples for several values of the trimming fraction $\alpha $. The training sample size used is $0.5n$.}}{29}{figure.11}}
\newlabel{fig:tlmbyAlpha}{{11}{29}{Average TLM plus/minus one standard deviation over $K = 50$ splits into training and holdout samples for several values of the trimming fraction $\alpha $. The training sample size used is $0.5n$}{figure.11}{}}
\citation{gelman2006}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Hierarchical model results: $\overline  {TLM}_b(A)$ plus/minus one standard deviation over $K = 50$ splits into training and holdout sets for several values of the trimming fraction $\alpha $.}}{31}{figure.12}}
\newlabel{fig:hierTLM}{{12}{31}{Hierarchical model results: $\overline {TLM}_b(A)$ plus/minus one standard deviation over $K = 50$ splits into training and holdout sets for several values of the trimming fraction $\alpha $}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Hierarchical model results: standard deviation of $\overline  {TLM}_b(A)$ relative to the mean over $K = 50$ splits into training and holdout sets for several values of the trimming fraction $\alpha $. Results are given for the two restricted likelihood versions of the hierarchical model and their corresponding robust regression models fit separately within each state.}}{32}{figure.13}}
\newlabel{fig:hierTLMsd}{{13}{32}{Hierarchical model results: standard deviation of $\overline {TLM}_b(A)$ relative to the mean over $K = 50$ splits into training and holdout sets for several values of the trimming fraction $\alpha $. Results are given for the two restricted likelihood versions of the hierarchical model and their corresponding robust regression models fit separately within each state}{figure.13}{}}
\@writefile{brf}{\backcite{gelman2006}{{33}{6.2}{figure.13}}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{33}{section.7}}
\newlabel{Conclusions}{{7}{33}{Discussion}{section.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Appendix}{34}{section.8}}
\newlabel{sec:appendix}{{8}{34}{Appendix}{section.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Proofs}{34}{subsection.8.1}}
\newlabel{perpGradReg}{{8.1}{35}{Proofs}{equation.8.21}{}}
\newlabel{eq:lem3.2}{{26}{35}{Proofs}{equation.8.26}{}}
\newlabel{BigMatrix}{{27}{35}{Proofs}{equation.8.27}{}}
\citation{miao1992}
\@writefile{brf}{\backcite{miao1992}{{36}{8.1}{equation.8.27}}}
\newlabel{MBI:lemma}{{8.1}{36}{}{theorem.8.1}{}}
\newlabel{MBI:thm}{{8.2}{36}{}{theorem.8.2}{}}
\bibstyle{bib/ba}
\bibdata{bib/refPaper1}
\bibcite{berger2006}{{1}{2006}{{Berger}}{{}}}
\bibcite{clarke1995}{{2}{1995}{{Clarke and Ghosh}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Setting the hierarchical prior values}{37}{subsection.8.2}}
\@writefile{toc}{\contentsline {section}{References}{37}{section*.6}}
\bibcite{doksum1990}{{3}{1990}{{Doksum and Lo}}{{}}}
\bibcite{fearnhead2012}{{4}{2012}{{Fearnhead and Prangle}}{{}}}
\bibcite{garthwaite2005}{{5}{2005}{{Garthwaite et~al.}}{{Garthwaite, Kadane, and O'Hagan}}}
\bibcite{gelfand1990}{{6}{1990}{{Gelfand and Smith}}{{}}}
\bibcite{gelman2006}{{7}{2006}{{Gelman}}{{}}}
\bibcite{hastings1970}{{8}{1970}{{Hastings}}{{}}}
\bibcite{hoff2013}{{9}{2013}{{Hoff et~al.}}{{Hoff, Fosdick, Volfovsky, and Stovel}}}
\bibcite{huber2009}{{10}{2009}{{Huber and Ronchetti}}{{}}}
\bibcite{huber1964}{{11}{1964}{{Huber}}{{}}}
\bibcite{hwang2005}{{12}{2005}{{Hwang et~al.}}{{Hwang, So, and Kim}}}
\bibcite{joyce2008}{{13}{2008}{{Joyce and Marjoram}}{{}}}
\bibcite{jung2014}{{14}{2014}{{Jung et~al.}}{{Jung, MacEachern, and Lee}}}
\bibcite{kass1995}{{15}{1995}{{Kass and Raftery}}{{}}}
\bibcite{kass1995reference}{{16}{1995}{{Kass and Wasserman}}{{}}}
\bibcite{lee2014}{{17}{2014}{{Lee and MacEachern}}{{}}}
\bibcite{lewis2014}{{18}{2014}{{Lewis}}{{}}}
\bibcite{lewis2012}{{19}{2012}{{Lewis et~al.}}{{Lewis, Lee, and MacEachern}}}
\bibcite{liang2008}{{20}{2008}{{Liang et~al.}}{{Liang, Paulo, Molina, Clyde, and Berger}}}
\bibcite{liu1994}{{21}{1994}{{Liu}}{{}}}
\bibcite{marjoram2003}{{22}{2003}{{Marjoram et~al.}}{{Marjoram, Molitor, Plagnol, and Tavar\'{e}}}}
\bibcite{maronna2006}{{23}{2006}{{Maronna et~al.}}{{Maronna, Martin, and Yohai}}}
\bibcite{miao1992}{{24}{1992}{{Miao and Ben-Israel}}{{}}}
\bibcite{ohagan2006}{{25}{2006}{{O'Hagan et~al.}}{{O'Hagan, Buck, Daneshkhah, Eiser, Garthwaite, Jenkinson, Oakley, and Rakow}}}
\bibcite{pettitt1982}{{26}{1982}{{Pettitt}}{{}}}
\bibcite{pettitt1983}{{27}{1983}{{Pettitt}}{{}}}
\bibcite{pratt1965}{{28}{1965}{{Pratt}}{{}}}
\bibcite{pritchard1999}{{29}{1999}{{Pritchard et~al.}}{{Pritchard, Seielstad, Perez-Lezaun, and Feldman}}}
\bibcite{ratcliff1993}{{30}{1993}{{Ratcliff}}{{}}}
\bibcite{ronchetti1997}{{31}{1997}{{Ronchetti et~al.}}{{Ronchetti, Field, and Blanchard}}}
\bibcite{savage1969}{{32}{1969}{{Savage}}{{}}}
\bibcite{tavare1997}{{33}{1997}{{Tavar{\'e} et~al.}}{{Tavar{\'e}, Balding, Griffiths, and Donnelly}}}
\bibcite{yuan2004}{{34}{2004}{{Yuan and Clarke}}{{}}}
\bibcite{zellner1986}{{35}{1986}{{Zellner}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Hierarchical model results: ${TLM}_b(A)_{j}$ plus/minus one standard deviation over $K = 50$ repetitions for each state and $\alpha = 0.3$. The states are ordered along the x-axis according to number of agencies with the state (shown in parentheses). Results displayed are for the robust models using Tukey's M-estimators.}}{42}{figure.14}}
\newlabel{fig:hierTLMstate}{{14}{42}{Hierarchical model results: ${TLM}_b(A)_{j}$ plus/minus one standard deviation over $K = 50$ repetitions for each state and $\alpha = 0.3$. The states are ordered along the x-axis according to number of agencies with the state (shown in parentheses). Results displayed are for the robust models using Tukey's M-estimators}{figure.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Hierarchical model results: Standard deviation of ${TLM}_b(A)_{j}$ relative to the mean over $K = 50$ repetitions for each state and $\alpha = 0.3$. The states are ordered along the x-axis according to number of agencies with the state (shown in parentheses). Results displayed are for the robust models using Tukey's M-estimators.}}{43}{figure.15}}
\newlabel{fig:hierTLMstateSD}{{15}{43}{Hierarchical model results: Standard deviation of ${TLM}_b(A)_{j}$ relative to the mean over $K = 50$ repetitions for each state and $\alpha = 0.3$. The states are ordered along the x-axis according to number of agencies with the state (shown in parentheses). Results displayed are for the robust models using Tukey's M-estimators}{figure.15}{}}
