\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{garthwaite2005,ohagan2006}
\citation{berger2006}
\citation{kass1995}
\gdef\hy@title{Bayesian Restricted Likelihood Methods: Conditioning on Insufficient Statistics in Bayesian Regression}
\thanksnewlabel{T1thanks}{{\TextOrMath  {\textasteriskcentered }{*}}{1}}
\thanksnewlabel{e1@email}{{lewis.865@osu.edu}{1}}
\thanksnewlabel{e2@email}{{snm@stat.osu.edu}{1}}
\thanksnewlabel{e3@email}{{yklee@stat.osu.edu}{1}}
\thanksnewlabel{addr1thanks}{{\TextOrMath  {\textdagger }{\dagger }}{1}}
\gdef\hy@author{John R. Lewis, Steven N. MacEachern and Yoonkyung Lee}
\gdef\hy@subject{Bayesian Analysis0000 0001}
\gdef\hy@keywords{Markov chain Monte Carlo, M-estimation, Robust regression}
\gdef\author@num{3}
\citation{yuan1999minimally}
\citation{zhu2011}
\citation{clyde2004}
\citation{bernardo2000,clyde2013,clarke2013Complete}
\citation{lee2014}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\@writefile{brf}{\backcite{garthwaite2005}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{ohagan2006}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{berger2006}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{kass1995}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{yuan1999minimally}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{zhu2011}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{clyde2004}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{bernardo2000}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{clyde2013}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{clarke2013Complete}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{lee2014}{{2}{1}{section.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Restricted Likelihood}{3}{section.2}}
\newlabel{restrictedlikelihood}{{2}{3}{Restricted Likelihood}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Examples}{3}{subsection.2.1}}
\citation{ratcliff1993}
\citation{lewis2014}
\newlabel{OutlyingCases}{{1}{4}{Examples}{equation.2.1}{}}
\@writefile{brf}{\backcite{ratcliff1993}{{4}{2.1}{equation.2.1}}}
\newlabel{Censoring}{{2}{4}{Examples}{equation.2.2}{}}
\@writefile{brf}{\backcite{lewis2014}{{4}{2.1}{equation.2.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Generalization}{4}{subsection.2.2}}
\newlabel{FullLikelihood}{{3}{4}{Generalization}{equation.2.3}{}}
\citation{wong2004}
\citation{savage1969}
\citation{pettitt1983,pettitt1982}
\citation{hoff2013}
\citation{lewis2012}
\citation{doksum1990}
\citation{clarke1995}
\citation{yuan2004}
\citation{hwang2005}
\citation{pratt1965}
\citation{tavare1997,pritchard1999,marjoram2003,fearnhead2012}
\citation{joyce2008}
\newlabel{RestrictedPosterior}{{4}{5}{Generalization}{equation.2.4}{}}
\newlabel{RestrictedpredDist}{{5}{5}{Generalization}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Literature review}{5}{subsection.2.3}}
\@writefile{brf}{\backcite{wong2004}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{savage1969}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{pettitt1983}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{pettitt1982}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{hoff2013}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{lewis2012}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{doksum1990}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{clarke1995}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{yuan2004}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{hwang2005}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{pratt1965}{{5}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{tavare1997}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{pritchard1999}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{marjoram2003}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{fearnhead2012}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{joyce2008}{{6}{2.3}{subsection.2.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Illustrative Examples}{6}{section.3}}
\newlabel{illustrations}{{3}{6}{Illustrative Examples}{section.3}{}}
\citation{huber2009}
\citation{kass1995reference}
\@writefile{brf}{\backcite{huber2009}{{7}{3}{equation.3.6}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Results from the analysis of the speed of light data. Top: Posterior distributions of $\beta $ under each model. Bottom: Log posterior predictive distributions under each model.}}{8}{figure.1}}
\newlabel{fig:newcomb_post}{{1}{8}{Results from the analysis of the speed of light data. Top: Posterior distributions of $\beta $ under each model. Bottom: Log posterior predictive distributions under each model}{figure.1}{}}
\@writefile{brf}{\backcite{kass1995reference}{{8}{3}{equation.3.7}}}
\citation{huber1964}
\citation{lewis2014}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Predictive distribution of log(calls) under the Normal theory model fit to the non-outliers, the restricted likelihood model with Tukey's M-estimator for the slope and intercept with Huber's `proposal 2' for scale, and a heavy-tailed t-distribution model. The first three data points were used to specify the prior with the remaining points used in the posterior fits. See details in the Appendix.}}{9}{figure.2}}
\newlabel{fig:calls_predictive}{{2}{9}{Predictive distribution of log(calls) under the Normal theory model fit to the non-outliers, the restricted likelihood model with Tukey's M-estimator for the slope and intercept with Huber's `proposal 2' for scale, and a heavy-tailed t-distribution model. The first three data points were used to specify the prior with the remaining points used in the posterior fits. See details in the Appendix}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Restricted Likelihood for the Linear Model}{10}{section.4}}
\newlabel{BayesLinMod}{{4}{10}{Restricted Likelihood for the Linear Model}{section.4}{}}
\@writefile{brf}{\backcite{huber1964}{{10}{4}{section.4}}}
\@writefile{brf}{\backcite{lewis2014}{{10}{4}{section.4}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}The Bayesian linear model}{10}{subsection.4.1}}
\newlabel{LinearModel}{{8}{10}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{fullRank}{{C1}{10}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{supReal}{{C2}{10}{The Bayesian linear model}{equation.4.8}{}}
\citation{huber2009,maronna2006}
\citation{lewis2014}
\citation{gelfand1990}
\citation{liu1994,liang2008}
\newlabel{asb}{{C3}{11}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{as}{{C4}{11}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{regEq}{{C5}{11}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{scaleEqReg}{{C6}{11}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{regIn}{{C7}{11}{The Bayesian linear model}{equation.4.8}{}}
\newlabel{scaleEq2Reg}{{C8}{11}{The Bayesian linear model}{equation.4.8}{}}
\@writefile{brf}{\backcite{huber2009}{{11}{4.1}{equation.4.8}}}
\@writefile{brf}{\backcite{maronna2006}{{11}{4.1}{equation.4.8}}}
\@writefile{brf}{\backcite{lewis2014}{{11}{4.1}{equation.4.8}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Computational strategy}{11}{subsection.4.2}}
\newlabel{highDim}{{4.2}{11}{Computational strategy}{subsection.4.2}{}}
\@writefile{brf}{\backcite{gelfand1990}{{11}{4.2}{subsection.4.2}}}
\citation{hastings1970}
\@writefile{brf}{\backcite{liu1994}{{12}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{liang2008}{{12}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{hastings1970}{{12}{4.2}{subsection.4.2}}}
\newlabel{MHRatio}{{9}{12}{Computational strategy}{equation.4.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Construction of the proposal}{12}{section*.1}}
\newlabel{Transformation}{{4.1}{13}{}{theorem.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluation of the proposal density}{14}{section*.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A depiction of $\mathcal  {A}$, $\Pi (\mathcal  {A})$, and the unit circle for the illustrative example where $b_{1}(\mathbf  {1},\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$y$})=\qopname  \relax m{min}(\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$y$})=0$ and $s(\mathbf  {1},\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$y$})=\DOTSB \sum@ \slimits@ (y_i -b_{1}(\mathbf  {1},\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$y$}))^2 =1$. $\mathcal  {A}$ is the combination of three quarter circles, one on each plane defined by $y_i=0$. The projection of this manifold onto the deviation space is depicted by the bowed triangular shape in the plane defined by $\DOTSB \sum@ \slimits@ y_i=0$. The circle in this plane represents the sample space for the intermediate sample $\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$z$}^*$. Also depicted is the vector $\mathbf  {1}$, the design matrix for the location and scale setting.}}{15}{figure.3}}
\newlabel{fig:sampSpace}{{3}{15}{A depiction of $\mathcal {A}$, $\Pi (\mathcal {A})$, and the unit circle for the illustrative example where $b_{1}(\mathbf {1},\by )=\min (\by )=0$ and $s(\mathbf {1},\by )=\sum (y_i -b_{1}(\mathbf {1},\by ))^2 =1$. $\mathcal {A}$ is the combination of three quarter circles, one on each plane defined by $y_i=0$. The projection of this manifold onto the deviation space is depicted by the bowed triangular shape in the plane defined by $\sum y_i=0$. The circle in this plane represents the sample space for the intermediate sample $\bz ^*$. Also depicted is the vector $\mathbf {1}$, the design matrix for the location and scale setting}{figure.3}{}}
\newlabel{gradSTheoremReg}{{4.2}{15}{}{theorem.4.2}{}}
\newlabel{fig:stretchDeform}{{4.2}{16}{Evaluation of the proposal density}{equation.4.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Visualization of the scaling from $z^{*}$ to $z$. Left: the first substep scales $z^{*}$ on the unit circle to the circle of radius $r = ||z||$; resulting in a change-of-variables transformation for the unit circle to that of radius $r$. The contribution to the Jacobian of this transformation is $r^{-(n-p-1)}$. Right: The second substep accounts for the the change-of-variables transformation from the circle of radius $r$ to $\Pi (\mathcal  {A})$. The normal vectors to these two sets are used to calculated the contribution to the Jacobian of this part of the transformation are shown in the figure.}}{16}{figure.4}}
\newlabel{cosine}{{12}{16}{Evaluation of the proposal density}{equation.4.12}{}}
\citation{miao1992}
\newlabel{lem:basis}{{4.3}{17}{}{theorem.4.3}{}}
\newlabel{lem:fullrank}{{4.4}{17}{}{theorem.4.4}{}}
\newlabel{eq:volume}{{13}{17}{Evaluation of the proposal density}{equation.4.13}{}}
\@writefile{brf}{\backcite{miao1992}{{17}{4.2}{equation.4.13}}}
\newlabel{Jacobian}{{4.5}{17}{}{theorem.4.5}{}}
\citation{miao1992}
\newlabel{dens:ystst}{{14}{18}{}{equation.4.14}{}}
\@writefile{brf}{\backcite{miao1992}{{18}{4.2}{equation.4.14}}}
\newlabel{theorem:sings}{{4.7}{18}{}{theorem.4.7}{}}
\newlabel{Mest}{{15}{18}{Evaluation of the proposal density}{equation.4.15}{}}
\citation{huber2009}
\@writefile{toc}{\contentsline {section}{\numberline {5}Simulated Data}{19}{section.5}}
\newlabel{simData}{{5}{19}{Simulated Data}{section.5}{}}
\newlabel{gensim2}{{16}{19}{Simulated Data}{equation.5.16}{}}
\newlabel{fullsim2}{{17}{19}{Simulated Data}{equation.5.17}{}}
\@writefile{brf}{\backcite{huber2009}{{19}{5}{equation.5.17}}}
\citation{huber2009}
\newlabel{kl}{{18}{20}{Simulated Data}{equation.5.18}{}}
\newlabel{kl_sim}{{5}{21}{Simulated Data}{equation.5.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Average KL - divergence plus/minus one standard error for each value of $a_{s}$ and $c$. The panels correspond to $c = 0.5$ (left), $c=1$ (middle), and $c=2$ (right) with the values of $a_{s}$ on the horizontal axis.}}{21}{figure.5}}
\@writefile{brf}{\backcite{huber2009}{{21}{5}{equation.5.18}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Average KL - divergence plus/minus one standard error grouped by the factors $m$ (left), $n$ (middle), and $p$ (right)}}{22}{figure.6}}
\newlabel{kl_mnp}{{6}{22}{Average KL - divergence plus/minus one standard error grouped by the factors $m$ (left), $n$ (middle), and $p$ (right)}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Boxplots $(\theta _{i} - \mathaccentV {hat}05E\theta _{i})$ across all simulations separated by the values for $m$ (left), $n$ (middle), $p$ (right) where $\mathaccentV {hat}05E\theta _{i}$ are the classical robust estimators (Huber's and Tukey's).}}{23}{figure.7}}
\newlabel{boxTheta}{{7}{23}{Boxplots $(\theta _{i} - \hat \theta _{i})$ across all simulations separated by the values for $m$ (left), $n$ (middle), $p$ (right) where $\hat \theta _{i}$ are the classical robust estimators (Huber's and Tukey's)}{figure.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Real Data}{23}{section.6}}
\newlabel{RealData}{{6}{23}{Real Data}{section.6}{}}
\citation{zellner1986,liang2008}
\citation{kass1995reference}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Boxplots of the classical robust estimators (Huber's and Tukey's) for $\sigma _{i}$ across all simulations separated by the values for $m$ (left), $n$ (middle), $p$ (right). The horizontal line at $\sigma = 2$ highlights the true standard deviation of the `good' data.}}{24}{figure.8}}
\newlabel{boxSigma}{{8}{24}{Boxplots of the classical robust estimators (Huber's and Tukey's) for $\sigma _{i}$ across all simulations separated by the values for $m$ (left), $n$ (middle), $p$ (right). The horizontal line at $\sigma = 2$ highlights the true standard deviation of the `good' data}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}State Level Regression model}{24}{subsection.6.1}}
\newlabel{regModelNW}{{6.1}{24}{State Level Regression model}{subsection.6.1}{}}
\newlabel{eq:regModel}{{19}{24}{State Level Regression model}{equation.6.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The square root of count in 2012 versus that in 2010 for four states. The colors represent the varying contractual agreements as they stood in 2010 (`Type'). Agencies that closed during the 2010-2012 period are represented by the zero counts for 2012.}}{25}{figure.9}}
\newlabel{fig:ctVct}{{9}{25}{The square root of count in 2012 versus that in 2010 for four states. The colors represent the varying contractual agreements as they stood in 2010 (`Type'). Agencies that closed during the 2010-2012 period are represented by the zero counts for 2012}{figure.9}{}}
\@writefile{brf}{\backcite{zellner1986}{{25}{6.1}{equation.6.19}}}
\@writefile{brf}{\backcite{liang2008}{{25}{6.1}{equation.6.19}}}
\@writefile{brf}{\backcite{kass1995reference}{{25}{6.1}{equation.6.19}}}
\citation{ronchetti1997}
\citation{jung2014}
\@writefile{toc}{\contentsline {subsubsection}{Method of model comparison}{26}{section*.3}}
\@writefile{brf}{\backcite{ronchetti1997}{{26}{6.1}{section*.3}}}
\@writefile{brf}{\backcite{jung2014}{{26}{6.1}{section*.3}}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison of predictive performance}{27}{section*.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Hierarchical regression model}{28}{subsection.6.2}}
\newlabel{hierRegNW}{{6.2}{28}{Hierarchical regression model}{subsection.6.2}{}}
\newlabel{eq:hierModel}{{20}{28}{Hierarchical regression model}{equation.6.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Average TLM plus/minus one standard deviation over $K = 50$ splits into training and holdout samples. The panels are for the different states 2, 15, 27, and 36, with $n = 222, 40, 117,$ and $46$, respectively. The horizontal axis is the percent of $n$ used in each training set. The color corresponds to the fitting model. }}{29}{figure.10}}
\newlabel{fig:tlm}{{10}{29}{Average TLM plus/minus one standard deviation over $K = 50$ splits into training and holdout samples. The panels are for the different states 2, 15, 27, and 36, with $n = 222, 40, 117,$ and $46$, respectively. The horizontal axis is the percent of $n$ used in each training set. The color corresponds to the fitting model}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Average TLM plus/minus one standard deviation over $K = 50$ splits into training and holdout samples for several values of the trimming fraction $\alpha $. The training sample size used is $0.5n$.}}{30}{figure.11}}
\newlabel{fig:tlmbyAlpha}{{11}{30}{Average TLM plus/minus one standard deviation over $K = 50$ splits into training and holdout samples for several values of the trimming fraction $\alpha $. The training sample size used is $0.5n$}{figure.11}{}}
\citation{gelman2006}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Hierarchical model results: $\overline  {TLM}_b(A)_{\cdot }$ plus/minus one standard deviation over $K = 50$ splits into training and holdout sets with the Student-t as the base model and several values of the trimming fraction $\alpha $.}}{32}{figure.12}}
\newlabel{fig:hierTLM}{{12}{32}{Hierarchical model results: $\overline {TLM}_b(A)_{\cdot }$ plus/minus one standard deviation over $K = 50$ splits into training and holdout sets with the Student-t as the base model and several values of the trimming fraction $\alpha $}{figure.12}{}}
\@writefile{brf}{\backcite{gelman2006}{{32}{6.2}{figure.13}}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Hierarchical model results: standard deviation of $\overline  {TLM}_b(A)_{\cdot }$ relative to the mean over $K = 50$ splits into training and holdout sets for several values of the trimming fraction $\alpha $. Results are given for the two restricted likelihood versions of the hierarchical model and their corresponding robust regression models fit separately within each state.}}{33}{figure.13}}
\newlabel{fig:hierTLMsd}{{13}{33}{Hierarchical model results: standard deviation of $\overline {TLM}_b(A)_{\cdot }$ relative to the mean over $K = 50$ splits into training and holdout sets for several values of the trimming fraction $\alpha $. Results are given for the two restricted likelihood versions of the hierarchical model and their corresponding robust regression models fit separately within each state}{figure.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{33}{section.7}}
\newlabel{Conclusions}{{7}{33}{Discussion}{section.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Appendix}{35}{section.8}}
\newlabel{sec:appendix}{{8}{35}{Appendix}{section.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Proofs}{35}{subsection.8.1}}
\newlabel{perpGradReg}{{8.1}{36}{Proofs}{equation.8.21}{}}
\newlabel{eq:lem3.2}{{26}{36}{Proofs}{equation.8.26}{}}
\newlabel{BigMatrix}{{27}{36}{Proofs}{equation.8.27}{}}
\citation{miao1992}
\@writefile{brf}{\backcite{miao1992}{{37}{8.1}{equation.8.27}}}
\newlabel{MBI:lemma}{{8.1}{37}{}{theorem.8.1}{}}
\newlabel{MBI:thm}{{8.2}{37}{}{theorem.8.2}{}}
\bibstyle{bib/ba}
\bibdata{bib/refPaper1}
\bibcite{berger2006}{{1}{2006}{{Berger}}{{}}}
\bibcite{bernardo2000}{{2}{2000}{{Bernardo and Smith}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Setting the hierarchical prior values}{38}{subsection.8.2}}
\@writefile{toc}{\contentsline {section}{References}{38}{section*.6}}
\bibcite{clarke1995}{{3}{1995}{{Clarke and Ghosh}}{{}}}
\bibcite{clarke2013Complete}{{4}{2013}{{Clarke et~al.}}{{Clarke, Clarke, Yu et~al.}}}
\bibcite{clyde2004}{{5}{2004}{{Clyde and George}}{{}}}
\bibcite{clyde2013}{{6}{2013}{{Clyde and Iversen}}{{}}}
\bibcite{doksum1990}{{7}{1990}{{Doksum and Lo}}{{}}}
\bibcite{fearnhead2012}{{8}{2012}{{Fearnhead and Prangle}}{{}}}
\bibcite{garthwaite2005}{{9}{2005}{{Garthwaite et~al.}}{{Garthwaite, Kadane, and O'Hagan}}}
\bibcite{gelfand1990}{{10}{1990}{{Gelfand and Smith}}{{}}}
\bibcite{gelman2006}{{11}{2006}{{Gelman}}{{}}}
\bibcite{hastings1970}{{12}{1970}{{Hastings}}{{}}}
\bibcite{hoff2013}{{13}{2013}{{Hoff et~al.}}{{Hoff, Fosdick, Volfovsky, and Stovel}}}
\bibcite{huber2009}{{14}{2009}{{Huber and Ronchetti}}{{}}}
\bibcite{huber1964}{{15}{1964}{{Huber}}{{}}}
\bibcite{hwang2005}{{16}{2005}{{Hwang et~al.}}{{Hwang, So, and Kim}}}
\bibcite{joyce2008}{{17}{2008}{{Joyce and Marjoram}}{{}}}
\bibcite{jung2014}{{18}{2014}{{Jung et~al.}}{{Jung, MacEachern, and Lee}}}
\bibcite{kass1995}{{19}{1995}{{Kass and Raftery}}{{}}}
\bibcite{kass1995reference}{{20}{1995}{{Kass and Wasserman}}{{}}}
\bibcite{lee2014}{{21}{2014}{{Lee and MacEachern}}{{}}}
\bibcite{lewis2014}{{22}{2014}{{Lewis}}{{}}}
\bibcite{lewis2012}{{23}{2012}{{Lewis et~al.}}{{Lewis, Lee, and MacEachern}}}
\bibcite{liang2008}{{24}{2008}{{Liang et~al.}}{{Liang, Paulo, Molina, Clyde, and Berger}}}
\bibcite{liu1994}{{25}{1994}{{Liu}}{{}}}
\bibcite{marjoram2003}{{26}{2003}{{Marjoram et~al.}}{{Marjoram, Molitor, Plagnol, and Tavar\'{e}}}}
\bibcite{maronna2006}{{27}{2006}{{Maronna et~al.}}{{Maronna, Martin, and Yohai}}}
\bibcite{miao1992}{{28}{1992}{{Miao and Ben-Israel}}{{}}}
\bibcite{ohagan2006}{{29}{2006}{{O'Hagan et~al.}}{{O'Hagan, Buck, Daneshkhah, Eiser, Garthwaite, Jenkinson, Oakley, and Rakow}}}
\bibcite{pettitt1982}{{30}{1982}{{Pettitt}}{{}}}
\bibcite{pettitt1983}{{31}{1983}{{Pettitt}}{{}}}
\bibcite{pratt1965}{{32}{1965}{{Pratt}}{{}}}
\bibcite{pritchard1999}{{33}{1999}{{Pritchard et~al.}}{{Pritchard, Seielstad, Perez-Lezaun, and Feldman}}}
\bibcite{ratcliff1993}{{34}{1993}{{Ratcliff}}{{}}}
\bibcite{ronchetti1997}{{35}{1997}{{Ronchetti et~al.}}{{Ronchetti, Field, and Blanchard}}}
\bibcite{savage1969}{{36}{1969}{{Savage}}{{}}}
\bibcite{tavare1997}{{37}{1997}{{Tavar{\'e} et~al.}}{{Tavar{\'e}, Balding, Griffiths, and Donnelly}}}
\bibcite{wong2004}{{38}{2004}{{Wong and Clarke}}{{}}}
\bibcite{yuan2004}{{39}{2004}{{Yuan and Clarke}}{{}}}
\bibcite{yuan1999minimally}{{40}{1999}{{Yuan and Clarke}}{{}}}
\bibcite{zellner1986}{{41}{1986}{{Zellner}}{{}}}
\bibcite{zhu2011}{{42}{2011}{{Zhu et~al.}}{{Zhu, Ibrahim, and Tang}}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Hierarchical model results: ${TLM}_b(A)_{j}$ plus/minus one standard deviation over $K = 50$ repetitions for each state and $\alpha = 0.3$. The states are ordered along the $x$-axis according to number of agencies with the state (shown in parentheses). Results displayed are for the robust models using Tukey's M-estimators.}}{43}{figure.14}}
\newlabel{fig:hierTLMstate}{{14}{43}{Hierarchical model results: ${TLM}_b(A)_{j}$ plus/minus one standard deviation over $K = 50$ repetitions for each state and $\alpha = 0.3$. The states are ordered along the $x$-axis according to number of agencies with the state (shown in parentheses). Results displayed are for the robust models using Tukey's M-estimators}{figure.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Hierarchical model results: Standard deviation of ${TLM}_b(A)_{j}$ relative to the mean over $K = 50$ repetitions for each state and $\alpha = 0.3$. The states are ordered along the $x$-axis according to number of agencies with the state (shown in parentheses). Results displayed are for the robust models using Tukey's M-estimators.}}{44}{figure.15}}
\newlabel{fig:hierTLMstateSD}{{15}{44}{Hierarchical model results: Standard deviation of ${TLM}_b(A)_{j}$ relative to the mean over $K = 50$ repetitions for each state and $\alpha = 0.3$. The states are ordered along the $x$-axis according to number of agencies with the state (shown in parentheses). Results displayed are for the robust models using Tukey's M-estimators}{figure.15}{}}
