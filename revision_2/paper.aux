\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{garthwaite2005,ohagan2006}
\citation{berger2006}
\citation{kass1995}
\citation{yuan1999minimally}
\citation{zhu2011}
\citation{clyde2004}
\citation{bernardo2000,clyde2013,clarke2013Complete}
\citation{lee2014}
\gdef\hy@title{Bayesian Restricted Likelihood Methods: Conditioning on Insufficient Statistics in Bayesian Regression}
\thanksnewlabel{T1thanks}{{\TextOrMath  {\textasteriskcentered }{*}}{1}}
\thanksnewlabel{e1@email}{{lewis.865@buckeyemail.osu.edu}{1}}
\thanksnewlabel{e2@email}{{snm@stat.osu.edu}{1}}
\thanksnewlabel{e3@email}{{yklee@stat.osu.edu}{1}}
\thanksnewlabel{addr1thanks}{{\TextOrMath  {\textdagger }{\dagger }}{1}}
\gdef\hy@author{John R. Lewis, Steven N. MacEachern and Yoonkyung Lee}
\gdef\hy@subject{Bayesian Analysis0000 0001}
\gdef\hy@keywords{Markov chain Monte Carlo, M-estimation, Robust regression}
\gdef\author@num{3}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\@writefile{brf}{\backcite{garthwaite2005}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{ohagan2006}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{berger2006}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{kass1995}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{yuan1999minimally}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{zhu2011}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{clyde2004}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{bernardo2000}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{clyde2013}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{clarke2013Complete}{{2}{1}{section.1}}}
\@writefile{brf}{\backcite{lee2014}{{2}{1}{section.1}}}
\citation{ratcliff1993}
\@writefile{toc}{\contentsline {section}{\numberline {2}Restricted Likelihood}{4}{section.2}}
\newlabel{restrictedlikelihood}{{2}{4}{Restricted Likelihood}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Examples}{4}{subsection.2.1}}
\newlabel{OutlyingCases}{{1}{4}{Examples}{equation.2.1}{}}
\@writefile{brf}{\backcite{ratcliff1993}{{4}{2.1}{equation.2.1}}}
\citation{lewis2014}
\newlabel{Censoring}{{2}{5}{Examples}{equation.2.2}{}}
\@writefile{brf}{\backcite{lewis2014}{{5}{2.1}{equation.2.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Generalization}{5}{subsection.2.2}}
\newlabel{FullLikelihood}{{3}{5}{Generalization}{equation.2.3}{}}
\newlabel{RestrictedPosterior}{{4}{5}{Generalization}{equation.2.4}{}}
\citation{wong2004}
\citation{savage1969}
\citation{pettitt1983,pettitt1982}
\citation{hoff2013}
\citation{lewis2012}
\citation{doksum1990}
\citation{clarke1995}
\citation{yuan2004}
\citation{hwang2005}
\citation{pratt1965}
\citation{tavare1997,pritchard1999,marjoram2003,fearnhead2012}
\citation{joyce2008}
\newlabel{RestrictedpredDist}{{5}{6}{Generalization}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Literature review}{6}{subsection.2.3}}
\@writefile{brf}{\backcite{wong2004}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{savage1969}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{pettitt1983}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{pettitt1982}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{hoff2013}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{lewis2012}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{doksum1990}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{clarke1995}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{yuan2004}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{hwang2005}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{pratt1965}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{tavare1997}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{pritchard1999}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{marjoram2003}{{6}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{fearnhead2012}{{6}{2.3}{subsection.2.3}}}
\citation{stigler1977}
\citation{lee2014}
\citation{huber2009}
\citation{huber2009}
\@writefile{brf}{\backcite{joyce2008}{{7}{2.3}{subsection.2.3}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Illustrative Examples}{7}{subsection.2.4}}
\newlabel{illustrations}{{2.4}{7}{Illustrative Examples}{subsection.2.4}{}}
\@writefile{brf}{\backcite{stigler1977}{{7}{2.4}{subsection.2.4}}}
\@writefile{brf}{\backcite{lee2014}{{7}{2.4}{subsection.2.4}}}
\@writefile{brf}{\backcite{huber2009}{{7}{2.4}{equation.2.6}}}
\citation{rousseeuw1987}
\citation{kass1995reference}
\@writefile{brf}{\backcite{huber2009}{{8}{2.4}{equation.2.6}}}
\@writefile{brf}{\backcite{rousseeuw1987}{{8}{2.4}{figure.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Results from the analysis of the speed of light data. Top: Posterior distributions of $\beta $ under each model. Bottom: Log posterior predictive distributions under each model. The differences in the tails are emphasized in the bottom plot. The horizontal axis is strategically labeled to help compare the centers of the distributions in each of the plots.}}{9}{figure.1}}
\newlabel{fig:newcomb_post}{{1}{9}{Results from the analysis of the speed of light data. Top: Posterior distributions of $\beta $ under each model. Bottom: Log posterior predictive distributions under each model. The differences in the tails are emphasized in the bottom plot. The horizontal axis is strategically labeled to help compare the centers of the distributions in each of the plots}{figure.1}{}}
\@writefile{brf}{\backcite{kass1995reference}{{9}{2.4}{equation.2.7}}}
\citation{huber1964}
\citation{lewis2014}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Pointwise posterior predictive intervals of log(calls) under the normal theory model fit to the non-outliers, the restricted likelihood model with Tukey's M-estimator for the slope and intercept with Huber's `proposal 2' for scale, and a heavy-tailed t-distribution model. The first three data points were used to specify the prior with each model using the remaining 21 for fitting. The normal theory model was also fit after removing observations 14-20 (years 1963 - 1970).}}{11}{figure.2}}
\newlabel{fig:calls_predictive}{{2}{11}{Pointwise posterior predictive intervals of log(calls) under the normal theory model fit to the non-outliers, the restricted likelihood model with Tukey's M-estimator for the slope and intercept with Huber's `proposal 2' for scale, and a heavy-tailed t-distribution model. The first three data points were used to specify the prior with each model using the remaining 21 for fitting. The normal theory model was also fit after removing observations 14-20 (years 1963 - 1970)}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Restricted Likelihood for the Linear Model}{11}{section.3}}
\newlabel{BayesLinMod}{{3}{11}{Restricted Likelihood for the Linear Model}{section.3}{}}
\@writefile{brf}{\backcite{huber1964}{{11}{3}{section.3}}}
\@writefile{brf}{\backcite{lewis2014}{{11}{3}{section.3}}}
\citation{huber2009,maronna2006}
\citation{lewis2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Bayesian linear model}{12}{subsection.3.1}}
\newlabel{LinearModel}{{8}{12}{The Bayesian linear model}{equation.3.8}{}}
\newlabel{fullRank}{{C1}{12}{The Bayesian linear model}{equation.3.8}{}}
\newlabel{supReal}{{C2}{12}{The Bayesian linear model}{equation.3.8}{}}
\newlabel{asb}{{C3}{12}{The Bayesian linear model}{equation.3.8}{}}
\newlabel{as}{{C4}{12}{The Bayesian linear model}{equation.3.8}{}}
\newlabel{regEq}{{C5}{12}{The Bayesian linear model}{equation.3.8}{}}
\newlabel{scaleEqReg}{{C6}{12}{The Bayesian linear model}{equation.3.8}{}}
\newlabel{regIn}{{C7}{12}{The Bayesian linear model}{equation.3.8}{}}
\newlabel{scaleEq2Reg}{{C8}{12}{The Bayesian linear model}{equation.3.8}{}}
\@writefile{brf}{\backcite{huber2009}{{12}{3.1}{equation.3.8}}}
\citation{gelfand1990}
\citation{liu1994,liang2008}
\citation{hastings1970}
\@writefile{brf}{\backcite{maronna2006}{{13}{3.1}{equation.3.8}}}
\@writefile{brf}{\backcite{lewis2014}{{13}{3.1}{equation.3.8}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Computational strategy}{13}{subsection.3.2}}
\newlabel{highDim}{{3.2}{13}{Computational strategy}{subsection.3.2}{}}
\@writefile{brf}{\backcite{gelfand1990}{{13}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{liu1994}{{13}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{liang2008}{{13}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{hastings1970}{{13}{3.2}{subsection.3.2}}}
\newlabel{MHRatio}{{9}{14}{Computational strategy}{equation.3.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Construction of the proposal}{14}{section*.1}}
\newlabel{Transformation}{{3.1}{14}{}{theorem.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A depiction of $\mathcal  {A}$, $\Pi (\mathcal  {A})$, and the unit circle for the illustrative example where $b_{1}(\mathbf  {1},\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$y$})=\qopname  \relax m{min}(\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$y$})=0$ and $s(\mathbf  {1},\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$y$})=\DOTSB \sum@ \slimits@ (y_i -b_{1}(\mathbf  {1},\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$y$}))^2 =1$. $\mathcal  {A}$ is the combination of three quarter circles, one on each plane defined by $y_i=0$. The projection of this manifold onto the deviation space is depicted by the bowed triangular shape in the plane defined by $\DOTSB \sum@ \slimits@ y_i=0$. The circle in this plane represents the sample space for the intermediate sample $\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$z$}^*$. Also depicted is the vector $\mathbf  {1}$, the design matrix for the location and scale setting.}}{16}{figure.3}}
\newlabel{fig:sampSpace}{{3}{16}{A depiction of $\mathcal {A}$, $\Pi (\mathcal {A})$, and the unit circle for the illustrative example where $b_{1}(\mathbf {1},\by )=\min (\by )=0$ and $s(\mathbf {1},\by )=\sum (y_i -b_{1}(\mathbf {1},\by ))^2 =1$. $\mathcal {A}$ is the combination of three quarter circles, one on each plane defined by $y_i=0$. The projection of this manifold onto the deviation space is depicted by the bowed triangular shape in the plane defined by $\sum y_i=0$. The circle in this plane represents the sample space for the intermediate sample $\bz ^*$. Also depicted is the vector $\mathbf {1}$, the design matrix for the location and scale setting}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluation of the proposal density}{17}{section*.2}}
\newlabel{gradSTheoremReg}{{3.2}{17}{}{theorem.3.2}{}}
\newlabel{cosine}{{12}{17}{Evaluation of the proposal density}{equation.3.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Visualization of the scaling from $z^{*}$ to $z$. Left: the first substep scales $z^{*}$ on the unit circle to the circle of radius $r = ||z||$, resulting in a change-of-variables transformation for the unit circle to a circle of radius $r$. The contribution to the Jacobian of this transformation is $r^{-(n-p-1)}$. Right: The second substep accounts for the the change-of-variables transformation from the circle of radius $r$ to $\Pi (\mathcal  {A})$. The normal vectors to these two sets are used to calculate the contribution to the Jacobian of this part of the transformation are shown in the figure.}}{18}{figure.4}}
\newlabel{fig:stretchDeform}{{4}{18}{Visualization of the scaling from $z^{*}$ to $z$. Left: the first substep scales $z^{*}$ on the unit circle to the circle of radius $r = ||z||$, resulting in a change-of-variables transformation for the unit circle to a circle of radius $r$. The contribution to the Jacobian of this transformation is $r^{-(n-p-1)}$. Right: The second substep accounts for the the change-of-variables transformation from the circle of radius $r$ to $\Pi (\mathcal {A})$. The normal vectors to these two sets are used to calculate the contribution to the Jacobian of this part of the transformation are shown in the figure}{figure.4}{}}
\newlabel{lem:basis}{{3.3}{18}{}{theorem.3.3}{}}
\citation{miao1992}
\newlabel{lem:fullrank}{{3.4}{19}{}{theorem.3.4}{}}
\newlabel{eq:volume}{{13}{19}{Evaluation of the proposal density}{equation.3.13}{}}
\@writefile{brf}{\backcite{miao1992}{{19}{3.2}{equation.3.13}}}
\newlabel{Jacobian}{{3.5}{19}{}{theorem.3.5}{}}
\newlabel{dens:ystst}{{14}{19}{}{equation.3.14}{}}
\citation{miao1992}
\@writefile{brf}{\backcite{miao1992}{{20}{3.2}{equation.3.14}}}
\newlabel{theorem:sings}{{3.7}{20}{}{theorem.3.7}{}}
\newlabel{Mest}{{15}{21}{Evaluation of the proposal density}{equation.3.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Simulated Data}{21}{section.4}}
\newlabel{simData}{{4}{21}{Simulated Data}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Simulation 1}{21}{subsection.4.1}}
\newlabel{gensim2}{{16}{21}{Simulation 1}{equation.4.16}{}}
\citation{huber2009}
\newlabel{fullsim2}{{17}{22}{Simulation 1}{equation.4.17}{}}
\@writefile{brf}{\backcite{huber2009}{{22}{4.1}{equation.4.17}}}
\newlabel{sim1loss}{{18}{23}{Simulation 1}{equation.4.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Average Loss plus/minus one standard error for each value of $a_{s}$ and $c$. Smaller values represent better fits. The panels correspond to $c = 0.5$ (left), $c=1$ (middle), and $c=2$ (right), with the values of $a_{s}$ on the horizontal axis. The average loss for the normal theory model ranges from $1.19$ to $1.42$ and is left out of the figure.}}{24}{figure.5}}
\newlabel{kl_sim}{{5}{24}{Average Loss plus/minus one standard error for each value of $a_{s}$ and $c$. Smaller values represent better fits. The panels correspond to $c = 0.5$ (left), $c=1$ (middle), and $c=2$ (right), with the values of $a_{s}$ on the horizontal axis. The average loss for the normal theory model ranges from $1.19$ to $1.42$ and is left out of the figure}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Average loss plus/minus one standard error grouped by the factors $m$ (left), $n$ (middle), and $p$ (right). These results are for the single prior with $a_s = 5$ and $c = 1$.}}{25}{figure.6}}
\newlabel{kl_mnp}{{6}{25}{Average loss plus/minus one standard error grouped by the factors $m$ (left), $n$ (middle), and $p$ (right). These results are for the single prior with $a_s = 5$ and $c = 1$}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}{\color  {blue}Simulation 2}}{25}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Average MSE plus/minus one standard error over the $K = 30$ simulations for each value of the prior standard deviation ($\sigma ^{2}_{\beta }$) and each of the fitting methods. `Restricted' is our method conditioning on Tukey's estimator of location and Huber's estimator of scale. `rlm' refers to the classical robust linear model fit with the same estimators and `t' is the heavy-tailed Bayesian model with a Student-t likelihood. The `rlm' results are the same for each $\sigma ^{2}_{\beta }$.}}{27}{figure.7}}
\newlabel{mseSimMany}{{7}{27}{Average MSE plus/minus one standard error over the $K = 30$ simulations for each value of the prior standard deviation ($\sigma ^{2}_{\beta }$) and each of the fitting methods. `Restricted' is our method conditioning on Tukey's estimator of location and Huber's estimator of scale. `rlm' refers to the classical robust linear model fit with the same estimators and `t' is the heavy-tailed Bayesian model with a Student-t likelihood. The `rlm' results are the same for each $\sigma ^{2}_{\beta }$}{figure.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Real Data}{27}{section.5}}
\newlabel{RealData}{{5}{27}{Real Data}{section.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Average MNLL plus/minus one standard error over the $K = 30$ simulations for each value of the prior standard deviation ($\sigma ^{2}_{\beta }$) and each of the fitting methods. `Restricted' is our method conditioning on Tukey's estimator of location and Huber's estimator of scale. `rlm' refers to the classical robust linear model fit with the same estimators and `t' is the heavy-tailed Bayesian model with a Student-t likelihood. The `rlm' results are the same for each $\sigma ^{2}_{\beta }$.}}{28}{figure.8}}
\newlabel{negllSimMany}{{8}{28}{Average MNLL plus/minus one standard error over the $K = 30$ simulations for each value of the prior standard deviation ($\sigma ^{2}_{\beta }$) and each of the fitting methods. `Restricted' is our method conditioning on Tukey's estimator of location and Huber's estimator of scale. `rlm' refers to the classical robust linear model fit with the same estimators and `t' is the heavy-tailed Bayesian model with a Student-t likelihood. The `rlm' results are the same for each $\sigma ^{2}_{\beta }$}{figure.8}{}}
\citation{zellner1986,liang2008}
\citation{kass1995reference}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The square root of (scaled) count in 2012 versus that in 2010 for four states. The colors represent the varying contractual agreements as they stood in 2010 (`Type'). Agencies that closed during the 2010-2012 period are represented by the zero counts for 2012.}}{29}{figure.9}}
\newlabel{fig:ctVct}{{9}{29}{The square root of (scaled) count in 2012 versus that in 2010 for four states. The colors represent the varying contractual agreements as they stood in 2010 (`Type'). Agencies that closed during the 2010-2012 period are represented by the zero counts for 2012}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}State Level Regression model}{29}{subsection.5.1}}
\newlabel{regModelNW}{{5.1}{29}{State Level Regression model}{subsection.5.1}{}}
\newlabel{eq:regModel}{{19}{29}{State Level Regression model}{equation.5.19}{}}
\citation{ronchetti1997}
\citation{jung2014}
\@writefile{brf}{\backcite{zellner1986}{{30}{5.1}{equation.5.19}}}
\@writefile{brf}{\backcite{liang2008}{{30}{5.1}{equation.5.19}}}
\@writefile{brf}{\backcite{kass1995reference}{{30}{5.1}{equation.5.19}}}
\@writefile{toc}{\contentsline {subsubsection}{Method of model comparison}{30}{section*.3}}
\@writefile{brf}{\backcite{ronchetti1997}{{31}{5.1}{section*.3}}}
\@writefile{brf}{\backcite{jung2014}{{31}{5.1}{section*.3}}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison of predictive performance}{31}{section*.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Average TLM plus/minus one standard deviation over $K = 50$ splits into training and holdout samples. The panels are for the different states 2, 15, 27, and 36, with $n = 222, 40, 117,$ and $46$, respectively. The horizontal axis is the percent of $n$ used in each training set. The color corresponds to the fitting model. Larger values of TLM are better.}}{33}{figure.10}}
\newlabel{fig:tlm}{{10}{33}{Average TLM plus/minus one standard deviation over $K = 50$ splits into training and holdout samples. The panels are for the different states 2, 15, 27, and 36, with $n = 222, 40, 117,$ and $46$, respectively. The horizontal axis is the percent of $n$ used in each training set. The color corresponds to the fitting model. Larger values of TLM are better}{figure.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Hierarchical regression model}{33}{subsection.5.2}}
\newlabel{hierRegNW}{{5.2}{33}{Hierarchical regression model}{subsection.5.2}{}}
\newlabel{eq:hierModel}{{20}{33}{Hierarchical regression model}{equation.5.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Average TLM plus/minus one standard deviation over $K = 50$ splits into training and holdout samples for several values of the trimming fraction $\alpha $. The training sample size used is $0.5n$. Larger values of TLM are better.}}{34}{figure.11}}
\newlabel{fig:tlmbyAlpha}{{11}{34}{Average TLM plus/minus one standard deviation over $K = 50$ splits into training and holdout samples for several values of the trimming fraction $\alpha $. The training sample size used is $0.5n$. Larger values of TLM are better}{figure.11}{}}
\citation{gelman2006}
\@writefile{brf}{\backcite{gelman2006}{{36}{5.2}{figure.12}}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{36}{section.6}}
\newlabel{Conclusions}{{6}{36}{Discussion}{section.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Hierarchical model results: $\overline  {TLM}_b(A)_{\cdot }$ plus/minus one standard deviation over $K = 50$ splits into training and holdout sets with the Student-t as the base model and several values of the trimming fraction $\alpha $. Larger values of TLM are better.}}{37}{figure.12}}
\newlabel{fig:hierTLM}{{12}{37}{Hierarchical model results: $\overline {TLM}_b(A)_{\cdot }$ plus/minus one standard deviation over $K = 50$ splits into training and holdout sets with the Student-t as the base model and several values of the trimming fraction $\alpha $. Larger values of TLM are better}{figure.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Supplementary Material}{39}{section.7}}
\newlabel{sec:appendix}{{7}{39}{Supplementary Material}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Practical Considerations for Using the Restricted Likelihood}{39}{subsection.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Proofs}{40}{subsection.7.2}}
\newlabel{1to1onto}{{7.1}{40}{}{theorem.7.1}{}}
\newlabel{perpGradReg}{{7.2}{41}{Proofs}{theorem.7.1}{}}
\newlabel{eq:lem3.2}{{27}{41}{Proofs}{equation.7.27}{}}
\newlabel{BigMatrix}{{28}{41}{Proofs}{equation.7.28}{}}
\citation{miao1992}
\@writefile{brf}{\backcite{miao1992}{{42}{7.2}{equation.7.28}}}
\newlabel{MBI:lemma}{{7.2}{42}{}{theorem.7.2}{}}
\newlabel{MBI:thm}{{7.3}{42}{}{theorem.7.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Setting the hierarchical prior values}{43}{subsection.7.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}M-estimators}{44}{subsection.7.4}}
\newlabel{sec:Mestimators}{{7.4}{44}{M-estimators}{subsection.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}M-estimators of Location}{44}{subsection.7.5}}
\newlabel{sec:Mlocation}{{7.5}{44}{M-estimators of Location}{subsection.7.5}{}}
\newlabel{locMod}{{29}{44}{M-estimators of Location}{equation.7.29}{}}
\newlabel{MrhoLoc}{{30}{44}{M-estimators of Location}{equation.7.30}{}}
\newlabel{MpsiLoc}{{31}{44}{M-estimators of Location}{equation.7.31}{}}
\newlabel{MpsiLocWeight}{{32}{44}{M-estimators of Location}{equation.7.32}{}}
\newlabel{weightedAve}{{33}{44}{M-estimators of Location}{equation.7.33}{}}
\citation{huber2009}
\citation{huber2009}
\citation{maronna2006}
\citation{huber2009}
\citation{maronna2006}
\newlabel{Huber}{{34}{45}{M-estimators of Location}{equation.7.34}{}}
\newlabel{Tukey}{{35}{45}{M-estimators of Location}{equation.7.35}{}}
\@writefile{brf}{\backcite{huber2009}{{45}{7.5}{equation.7.35}}}
\@writefile{brf}{\backcite{huber2009}{{45}{7.5}{equation.7.35}}}
\@writefile{brf}{\backcite{maronna2006}{{45}{7.5}{equation.7.35}}}
\@writefile{brf}{\backcite{huber2009}{{45}{7.5}{equation.7.35}}}
\@writefile{brf}{\backcite{maronna2006}{{45}{7.5}{equation.7.35}}}
\newlabel{eq:Epsi}{{36}{45}{M-estimators of Location}{equation.7.36}{}}
\citation{huber1964}
\newlabel{eq:asyNormM}{{37}{46}{M-estimators of Location}{equation.7.37}{}}
\newlabel{eq:Eff}{{38}{46}{M-estimators of Location}{equation.7.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}M-estimators of Scale}{46}{subsection.7.6}}
\newlabel{sec:Mscale}{{7.6}{46}{M-estimators of Scale}{subsection.7.6}{}}
\newlabel{scaleMod}{{39}{46}{M-estimators of Scale}{equation.7.39}{}}
\newlabel{eq:Msigmachi}{{40}{46}{M-estimators of Scale}{equation.7.40}{}}
\@writefile{brf}{\backcite{huber1964}{{47}{7.6}{equation.7.40}}}
\newlabel{HuberScale}{{41}{47}{M-estimators of Scale}{equation.7.41}{}}
\newlabel{madchi}{{42}{47}{M-estimators of Scale}{equation.7.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7}Simultaneous M-estimators of Location and Scale}{47}{subsection.7.7}}
\newlabel{sec:MlocScale}{{7.7}{47}{Simultaneous M-estimators of Location and Scale}{subsection.7.7}{}}
\newlabel{locScaleMod}{{43}{47}{Simultaneous M-estimators of Location and Scale}{equation.7.43}{}}
\newlabel{simM}{{44}{47}{Simultaneous M-estimators of Location and Scale}{equation.7.44}{}}
\newlabel{regM}{{45}{47}{Simultaneous M-estimators of Location and Scale}{equation.7.45}{}}
\bibstyle{bib/ba}
\bibdata{bib/refPaper1}
\bibcite{berger2006}{{1}{2006}{{Berger}}{{}}}
\bibcite{bernardo2000}{{2}{2000}{{Bernardo and Smith}}{{}}}
\bibcite{clarke1995}{{3}{1995}{{Clarke and Ghosh}}{{}}}
\bibcite{clarke2013Complete}{{4}{2013}{{Clarke et~al.}}{{Clarke, Clarke, Yu et~al.}}}
\bibcite{clyde2004}{{5}{2004}{{Clyde and George}}{{}}}
\bibcite{clyde2013}{{6}{2013}{{Clyde and Iversen}}{{}}}
\bibcite{doksum1990}{{7}{1990}{{Doksum and Lo}}{{}}}
\newlabel{simMreg}{{46}{48}{Simultaneous M-estimators of Location and Scale}{equation.7.46}{}}
\newlabel{ExpMreg}{{47}{48}{Simultaneous M-estimators of Location and Scale}{equation.7.47}{}}
\@writefile{toc}{\contentsline {section}{References}{48}{section*.6}}
\bibcite{fearnhead2012}{{8}{2012}{{Fearnhead and Prangle}}{{}}}
\bibcite{garthwaite2005}{{9}{2005}{{Garthwaite et~al.}}{{Garthwaite, Kadane, and O'Hagan}}}
\bibcite{gelfand1990}{{10}{1990}{{Gelfand and Smith}}{{}}}
\bibcite{gelman2006}{{11}{2006}{{Gelman}}{{}}}
\bibcite{hastings1970}{{12}{1970}{{Hastings}}{{}}}
\bibcite{hoff2013}{{13}{2013}{{Hoff et~al.}}{{Hoff, Fosdick, Volfovsky, and Stovel}}}
\bibcite{huber2009}{{14}{2009}{{Huber and Ronchetti}}{{}}}
\bibcite{huber1964}{{15}{1964}{{Huber}}{{}}}
\bibcite{hwang2005}{{16}{2005}{{Hwang et~al.}}{{Hwang, So, and Kim}}}
\bibcite{joyce2008}{{17}{2008}{{Joyce and Marjoram}}{{}}}
\bibcite{jung2014}{{18}{2014}{{Jung et~al.}}{{Jung, MacEachern, and Lee}}}
\bibcite{kass1995}{{19}{1995}{{Kass and Raftery}}{{}}}
\bibcite{kass1995reference}{{20}{1995}{{Kass and Wasserman}}{{}}}
\bibcite{lee2014}{{21}{2014}{{Lee and MacEachern}}{{}}}
\bibcite{lewis2014}{{22}{2014}{{Lewis}}{{}}}
\bibcite{lewis2012}{{23}{2012}{{Lewis et~al.}}{{Lewis, Lee, and MacEachern}}}
\bibcite{liang2008}{{24}{2008}{{Liang et~al.}}{{Liang, Paulo, Molina, Clyde, and Berger}}}
\bibcite{liu1994}{{25}{1994}{{Liu}}{{}}}
\bibcite{marjoram2003}{{26}{2003}{{Marjoram et~al.}}{{Marjoram, Molitor, Plagnol, and Tavar\'{e}}}}
\bibcite{maronna2006}{{27}{2006}{{Maronna et~al.}}{{Maronna, Martin, and Yohai}}}
\bibcite{miao1992}{{28}{1992}{{Miao and Ben-Israel}}{{}}}
\bibcite{ohagan2006}{{29}{2006}{{O'Hagan et~al.}}{{O'Hagan, Buck, Daneshkhah, Eiser, Garthwaite, Jenkinson, Oakley, and Rakow}}}
\bibcite{pettitt1982}{{30}{1982}{{Pettitt}}{{}}}
\bibcite{pettitt1983}{{31}{1983}{{Pettitt}}{{}}}
\bibcite{pratt1965}{{32}{1965}{{Pratt}}{{}}}
\bibcite{pritchard1999}{{33}{1999}{{Pritchard et~al.}}{{Pritchard, Seielstad, Perez-Lezaun, and Feldman}}}
\bibcite{ratcliff1993}{{34}{1993}{{Ratcliff}}{{}}}
\bibcite{ronchetti1997}{{35}{1997}{{Ronchetti et~al.}}{{Ronchetti, Field, and Blanchard}}}
\bibcite{rousseeuw1987}{{36}{1987}{{Rousseeuw and Leroy}}{{}}}
\bibcite{savage1969}{{37}{1969}{{Savage}}{{}}}
\bibcite{stigler1977}{{38}{1977}{{Stigler}}{{}}}
\bibcite{tavare1997}{{39}{1997}{{Tavar{\'e} et~al.}}{{Tavar{\'e}, Balding, Griffiths, and Donnelly}}}
\bibcite{wong2004}{{40}{2004}{{Wong and Clarke}}{{}}}
\bibcite{yuan2004}{{41}{2004}{{Yuan and Clarke}}{{}}}
\bibcite{yuan1999minimally}{{42}{1999}{{Yuan and Clarke}}{{}}}
\bibcite{zellner1986}{{43}{1986}{{Zellner}}{{}}}
\bibcite{zhu2011}{{44}{2011}{{Zhu et~al.}}{{Zhu, Ibrahim, and Tang}}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Hierarchical model results: ${TLM}_b(A)_{j}$ plus/minus one standard deviation over $K = 50$ repetitions for each state and $\alpha = 0.3$. The states are ordered along the $x$-axis according to number of agencies within the state (shown in parentheses). Results displayed are for the robust models using Tukey's M-estimators. Larger values of TLM are better.}}{53}{figure.13}}
\newlabel{fig:hierTLMstate}{{13}{53}{Hierarchical model results: ${TLM}_b(A)_{j}$ plus/minus one standard deviation over $K = 50$ repetitions for each state and $\alpha = 0.3$. The states are ordered along the $x$-axis according to number of agencies within the state (shown in parentheses). Results displayed are for the robust models using Tukey's M-estimators. Larger values of TLM are better}{figure.13}{}}
